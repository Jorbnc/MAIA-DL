{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorbnc/MAIA-DL/blob/master/Mini_Proyecto_2_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oa_MJEGQ6jTi",
      "metadata": {
        "id": "oa_MJEGQ6jTi"
      },
      "source": [
        "![Universidad_de_los_Andes_30.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sl89pic17iPU",
      "metadata": {
        "id": "sl89pic17iPU"
      },
      "source": [
        "<style>\n",
        "@import url('https://fonts.googleapis.com/css2?family=Latin+Modern+Roman:wght@400;700&display=swap');\n",
        "\n",
        "body, p, h1, h2, h3, h4, h5, h6, li {\n",
        "  font-family: 'Latin Modern Roman', serif;\n",
        "}\n",
        "code, pre {\n",
        "  font-family: 'Fira Mono', monospace;\n",
        "}\n",
        "</style>\n",
        "\n",
        "***\n",
        "\n",
        "# **Mini Proyecto 2, T√©cnicas de *Deep Learning*: Clasificaci√≥n de Sentimientos de Rese√±as de Pel√≠culas en *IMDB* con Redes Neuronales Recurrentes**\n",
        "\n",
        "## **Descripci√≥n del problema:**\n",
        "\n",
        "En este proyecto abordamos el problema de clasificaci√≥n de sentimiento en rese√±as de pel√≠culas. Partimos de un conjunto de datos extra√≠do de Kaggle que contiene rese√±as textuales (**`text`**) y una etiqueta binaria (**`label`**) que indica si la opini√≥n es positiva (**1**) o negativa (**0**). El reto consiste en dise√±ar un modelo de *Deep Learning*, concretamente una Red Neuronal Recurrente con capas LSTM, capaz de procesar secuencias de texto y predecir con alta precisi√≥n la polaridad de cada rese√±a.\n",
        "\n",
        "## **Objetivo:**\n",
        "\n",
        "* Implementar un *pipeline* completo que incluya la descarga del dataset, el preprocesamiento de texto (tokenizaci√≥n, limpieza, construcci√≥n de vocabulario y padding de secuencias), y la definici√≥n de un Dataset y DataLoader en PyTorch.\n",
        "\n",
        "* Dise√±ar una arquitectura basada en *embeddings* y una o varias capas LSTM (bidireccionales), con regularizaci√≥n por *dropout* y optimizaci√≥n con **`Adam`**.\n",
        "\n",
        "* Entrenar el modelo utilizando GPU cuando est√© disponible, incorporando t√©cnicas de *early stopping* y gradient clipping para evitar sobreajuste y explosi√≥n de gradientes.\n",
        "\n",
        "* Evaluar su desempe√±o final sobre el conjunto de prueba, calculando *accuracy*, *F1‚Äëscore*, *recall* y presentando la matriz de confusi√≥n.\n",
        "\n",
        "* Visualizar la evoluci√≥n de la p√©rdida y la exactitud en entrenamiento y validaci√≥n a lo largo de las √©pocas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SiCS5j-HAbCB",
      "metadata": {
        "id": "SiCS5j-HAbCB"
      },
      "source": [
        "***\n",
        "\n",
        "**Este proyecto es realizado por Andr√©s Felipe √ëungo y Jordan Bryan N√∫√±ez Campos para entrega el 9 de mayo.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# Tareas\n",
        "\n",
        "* Exploraci√≥n y pre-procesamiento (Jordan)\n",
        "\n",
        "* Introducci√≥n, y explicaci√≥n de las opciones tomadas en el PDF (Andr√©s y Jordan)\n",
        "\n",
        "* Comentarios en c√≥digo, explicaciones en markdowns (Andr√©s y Jordan)\n",
        "\n",
        "* Nube de Palabras (Jordan) ‚úÖ\n",
        "\n",
        "* Validaci√≥n de idiomas Jorda) ‚úÖ\n",
        "\n",
        "* Glove  (Jordan)\n",
        "\n",
        "* Word2vec (Andr√©s) ‚úÖ\n",
        "\n",
        "* LSTM o GRU  (Andr√©s) ‚úÖ\n",
        "\n",
        "* Mejorar red (Andr√©s)\n",
        "\n",
        "* Pensar en ideas visualizaci√≥n (Andr√©s y Jordan)\n",
        "- t-SNE\n",
        "\n",
        "* Entrenamiento y evaluaci√≥n de modelo (Andr√©s) ‚úÖ\n",
        "\n",
        "* Informe: Arquitectura, entrenamiento, evaluaci√≥n, an√°lisis cuantitativo (Andr√©s)\n",
        "\n",
        "* Informe: Exploraci√≥n, pre-procesamiento, an√°lisis cualitativo, discusi√≥n (Jordan)"
      ],
      "metadata": {
        "id": "aP4T8CvEqYMG"
      },
      "id": "aP4T8CvEqYMG"
    },
    {
      "cell_type": "markdown",
      "id": "8VqTMx7TYgjH",
      "metadata": {
        "id": "8VqTMx7TYgjH"
      },
      "source": [
        "\n",
        "***\n",
        "# **√çndice**\n",
        "\n",
        "El *notebook* abordar√° el proyecto de la siguiente manera:\n",
        "\n",
        "| üîπ | Secci√≥n        |\n",
        "|----|----------------|\n",
        "| 1Ô∏è‚É£ | **Instalaci√≥n y carga de librer√≠as** |\n",
        "| 2Ô∏è‚É£ | **An√°lisis exploratorio y preparaci√≥n de los datos**       |\n",
        "| 3Ô∏è‚É£ | **Definici√≥n de *pipelines* de procesamiento**          |\n",
        "| 3Ô∏è‚É£.1Ô∏è‚É£ | **Pipeline de preprocesamiento ...**   |\n",
        "| 4Ô∏è‚É£ | **Desarrollo del modelo RNN...**   |\n",
        "| 4Ô∏è‚É£.1Ô∏è‚É£ | **Hiperpar√°metros, partici√≥n y Dataloaders**   |\n",
        "| 4Ô∏è‚É£.2Ô∏è‚É£ | **Adaptaci√≥n a partir de la arquitectura ...*   |\n",
        "| 4Ô∏è‚É£.3Ô∏è‚É£ | **Entrenamiento, validaci√≥n y prueba**   |\n",
        "| 5Ô∏è‚É£ | **An√°lisis de resultados y conclusiones**   |\n",
        "| 6Ô∏è‚É£ | **Conclusi√≥n**   |\n",
        "| 7Ô∏è‚É£ | **Referencias**   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cyVuwlHB_W3",
      "metadata": {
        "id": "2cyVuwlHB_W3"
      },
      "source": [
        "***\n",
        "\n",
        "# 1. Instalaci√≥n y cargue de librer√≠as"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta celda se prepara el entorno y se importan las librer√≠as clave. Primero se instala **`kagglehub`**, √∫til en Coursera para descargar el conjunto de datos directamente desde Kaggle. Luego se eliminan versiones previas de **`NumPy`**, **`gensim`** y **`pandas`** para evitar choques, y se reinstalan **`NumPy 1.26.4`** y **`pandas 2.2.2`**, compatibles con la imagen est√°ndar de Google Colab y TensorFlow. Se a√±ade **`Cython`**, necesario para compilar extensiones en Cython, y se recompila **`gensim 4.3.3`** desde el c√≥digo fuente (opci√≥n **`--no-binary :all:`**) para que enlace correctamente con **`NumPy 1.26`**. Finalmente se desinstalan **`spaCy`** y **`thinc`**, ya que no se usar√°n y generan advertencias de dependencia.\n",
        "\n",
        "Despu√©s del ajuste de versiones se importan utilidades generales (**`os`**, **`random`**,**`time`**, **`NumPy`**, **`pandas`**), herramientas de PLN con **`NLTK`** (incluida la descarga de *stop-words*), m√≥dulos de **`PyTorch`** para construir y entrenar la red, m√©tricas de **`scikit-learn`** y utilidades de visualizaci√≥n con **`Matplotlib`** y **`Seaborn`**. Por √∫ltimo, se cargan los embeddings preentrenados **`word2vec-google-news-300`** mediante **`gensim.downloader`**, que proporcionan vectores de 300 dimensiones para inicializar la capa de embedding del modelo."
      ],
      "metadata": {
        "id": "DkjBiBB5rU6t"
      },
      "id": "DkjBiBB5rU6t"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --no-cache-dir \\\n",
        "    \"numpy==1.26.4\" \\\n",
        "    \"scipy==1.12.1\" \\\n",
        "    \"pandas==2.2.2\" \\\n",
        "    \"scikit-learn==1.6.1\" \\\n",
        "    \"gensim==4.3.3\" \\\n",
        "    kagglehub langdetect Cython tqdm nltk seaborn matplotlib pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l26426jMLCxE",
        "outputId": "f449e576-acb2-427e-d13b-d378d2d4d3e6"
      },
      "id": "l26426jMLCxE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m154.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m152.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires spacy<4, which is not installed.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m169.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m161.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m189.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m182.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires spacy<4, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Librer√≠as comunes\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Preprocesamiento\n",
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Librer√≠as NLP\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "# Ppalabras vac√≠as\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Modelado\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "# Embeddings pre-entrenados\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Evaluaci√≥n\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Librer√≠as para visualizaciones\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "osjmTJMvtxua",
        "outputId": "8b65c2e2-d06d-4050-e896-37e4826deb2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "osjmTJMvtxua",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3489e65490b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;31m# These were moved in 1.25 and may be deprecated eventually:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;34m\"ModuleDeprecationWarning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VisibleDeprecationWarning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;34m\"ComplexWarning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TooHardError\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AxisError\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se carga los *embeddings* de los *tokens* de los documentos, que nos permitir√°n tener un nuevo espacio de representaci√≥n m√°s enriquecido. M√°s adelante se explica con m√°s detalle cada tipo de *embedding*.\n"
      ],
      "metadata": {
        "id": "07ZBWlW6BudL"
      },
      "id": "07ZBWlW6BudL"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Carga Word2Vec\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")  # vocab_size ~3M, emb_dim=300"
      ],
      "metadata": {
        "id": "Nv1ZI8lUBs2b"
      },
      "id": "Nv1ZI8lUBs2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importa **`kagglehub`** y ejecuta **`dataset_download`** con el identificador **`yasserh/imdb-movie-ratings-sentiment-analysis`** para descargar el conjunto de datos de rese√±as de pel√≠culas IMDB. La funci√≥n guarda los archivos de manera local y devuelve la ruta absoluta, que se almacena en **`path`** y se muestra en pantalla mediante **`print`** para confirmar d√≥nde quedaron los datos."
      ],
      "metadata": {
        "id": "yHwealvATakn"
      },
      "id": "yHwealvATakn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga del dataset\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"yasserh/imdb-movie-ratings-sentiment-analysis\")\n",
        "print(\"Datos descargados en:\", path)"
      ],
      "metadata": {
        "id": "F4y-tzhMl2CC"
      },
      "id": "F4y-tzhMl2CC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque detecta si el entorno dispone de GPU y selecciona el **`device`** apropiado para PyTorch.  \n",
        "Primero se llama a **`torch.cuda.is_available()`**, que devuelve *True* si se ha asignado una GPU CUDA al runtime de Colab. Seg√∫n el resultado se imprime un mensaje informativo (‚ÄúEntrenando en GPU.‚Äù o ‚ÄúGPU no disponible, entrenando en CPU.‚Äù).  \n",
        "\n",
        "A continuaci√≥n se construye el objeto **`device`** con **`torch.device(\"cuda\" if train_on_gpu else \"cpu\")`**, que ser√° pasado a la red y a los tensores de entrada para que se ubiquen en la GPU cuando sea posible. Por √∫ltimo se muestra en pantalla el dispositivo elegido."
      ],
      "metadata": {
        "id": "uOceAejmVjqF"
      },
      "id": "uOceAejmVjqF"
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print(\"Entrenando en GPU.\")\n",
        "else:\n",
        "    print(\"GPU no disponible, entrenando en CPU.\")\n",
        "\n",
        "# 2) Elegir device seg√∫n disponibilidad\n",
        "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")\n",
        "print(f\"   Usando device: {device}\\n\")"
      ],
      "metadata": {
        "id": "LksJ95qEPWVr"
      },
      "id": "LksJ95qEPWVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque hace dos tareas sencillas de soporte:\n",
        "\n",
        "1. Llama a **`warnings.filterwarnings(\"ignore\")`** para ocultar avisos y mantener la salida del notebook limpia.\n",
        "2. Utiliza **`importlib.metadata.version`** para imprimir las versiones de un conjunto de librer√≠as clave ( **`numpy`**, **`pandas`**, **`torch`**, **`torchvision`**, **`scikit-learn`**, **`kagglehub`**, **`pillow`**, **`matplotlib`**, **`seaborn`** ). Mostrar estas versiones al inicio del notebook facilita la reproducibilidad y ayuda a depurar posibles conflictos de dependencias."
      ],
      "metadata": {
        "id": "pfXxKk09Vulp"
      },
      "id": "pfXxKk09Vulp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LSwlUgjUjtR_",
      "metadata": {
        "id": "LSwlUgjUjtR_"
      },
      "outputs": [],
      "source": [
        "# Ignorar las warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Versiones utilizadas\n",
        "from importlib.metadata import version\n",
        "librerias = [\n",
        "    \"numpy\", \"pandas\", \"torch\", \"torchvision\", \"scikit-learn\", \"kagglehub\",\n",
        "    \"pillow\", \"matplotlib\", \"seaborn\",\n",
        "    \"nltk\", \"gensim\", \"tqdm\", \"scipy\"\n",
        "]\n",
        "for library in librerias:\n",
        "  print(library, \": \", version(library))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente bloque de c√≥digo nos permite hacer determinin√≠sticas las funciones tra√≠das de **`pytorch`**, y en general controlar todos los pseudo-aleatorios del *notebook*"
      ],
      "metadata": {
        "id": "ok4F_5E1njTi"
      },
      "id": "ok4F_5E1njTi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definici√≥n del random state y seeds\n",
        "RANDOM_STATE = 13\n",
        "random.seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "metadata": {
        "id": "xdgcOt7gznvJ"
      },
      "id": "xdgcOt7gznvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qK311OLFzPep",
      "metadata": {
        "id": "qK311OLFzPep"
      },
      "source": [
        "***\n",
        "\n",
        "# 2. An√°lisis exploratorio y preparaci√≥n de los datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 2.1. Carga y estad√≠sticas generales"
      ],
      "metadata": {
        "id": "4KxKy7H1eID-"
      },
      "id": "4KxKy7H1eID-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4yOzyWKJbov",
      "metadata": {
        "id": "K4yOzyWKJbov"
      },
      "outputs": [],
      "source": [
        "# Descargar el conjunto de datos y almacenar el path en una variable\n",
        "data_raw = pd.read_csv(os.path.join(path, 'movie.csv'))\n",
        "data_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2index = {'Negativa':0, 'Positiva':1}\n",
        "index2label = {0:'Negativa', 1:'Positiva'}"
      ],
      "metadata": {
        "id": "tK_D4XVsnk6c"
      },
      "id": "tK_D4XVsnk6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frecuencias absoluta y relativa de cada clase\n",
        "pd.DataFrame({\n",
        "    \"Review\": index2label,\n",
        "    \"F. Absoluta\": data_raw['label'].value_counts().sort_index(),\n",
        "    \"F. Relativa\": data_raw['label'].value_counts(normalize=True).sort_index(),\n",
        "})"
      ],
      "metadata": {
        "id": "EkiUxk8il_JG"
      },
      "id": "EkiUxk8il_JG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "CNfurSnbmf79",
      "metadata": {
        "id": "CNfurSnbmf79"
      },
      "source": [
        "***\n",
        "\n",
        "## 2.2. Limpieza de los datos\n",
        "\n",
        "En estas secci√≥n identificamos y corregimos:\n",
        "\n",
        "* Valores faltantes\n",
        "* Textos duplicados\n",
        "* Textos en otros idiomas distintos al ingl√©s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw.isna().sum()"
      ],
      "metadata": {
        "id": "2r0P2t_Ysjbq"
      },
      "id": "2r0P2t_Ysjbq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw.duplicated().sum()"
      ],
      "metadata": {
        "id": "m3IgmRbWskuE"
      },
      "id": "m3IgmRbWskuE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_raw.drop_duplicates(keep='first')\n",
        "\n",
        "# Frecuencias absoluta y relativa de cada clase\n",
        "pd.DataFrame({\n",
        "    \"Review\": index2label,\n",
        "    \"F. Absoluta\": data['label'].value_counts().sort_index(),\n",
        "    \"F. Relativa\": data['label'].value_counts(normalize=True).sort_index(),\n",
        "})"
      ],
      "metadata": {
        "id": "37qzkcBL3D3P"
      },
      "id": "37qzkcBL3D3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traducir_todo_a_ingles(X: pd.DataFrame):\n",
        "  #Identificar los idiomas del dataframe y los √≠ndices de las filas que no est√°n en ingl√©s\n",
        "  X['idioma'] = X['text'].apply(lambda x: detect(x) if isinstance(x, str) and x.strip() else \"desconocido\")\n",
        "  indices_a_traducir = X[X['idioma'] != 'en'].index\n",
        "\n",
        "  # Imprimir textos en otros idiomas\n",
        "  print(\"Textos en otros idiomas:\")\n",
        "  for i in indices_a_traducir:\n",
        "    print(X.loc[i].text)\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "_WQ5tiJps-bO"
      },
      "id": "_WQ5tiJps-bO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar columna 'idioma' al df + ver textos en otros idiomas\n",
        "%time data = traducir_todo_a_ingles(data)\n",
        "data.idioma.value_counts()"
      ],
      "metadata": {
        "id": "17fC-8gv1YCD"
      },
      "id": "17fC-8gv1YCD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de esta secci√≥n es incorporar varios estilos de pre-procesamiento, probar diferentes maneras de depurar los textos como por ejemplo remover caracteres no alfanum√©ricos, a excepci√≥n de guiones o ap√≥strofes. Tambi√©n haremos uso de dos normalizadores: *Stemming* y *Lemmatization*, ...."
      ],
      "metadata": {
        "id": "vCm-sqa9t8D2"
      },
      "id": "vCm-sqa9t8D2"
    },
    {
      "cell_type": "markdown",
      "id": "_ypyorGqwmRl",
      "metadata": {
        "id": "_ypyorGqwmRl"
      },
      "source": [
        "***\n",
        "\n",
        "# 3. Definici√≥n de *pipelines* de procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4370a607-ad43-4c5d-bddd-1a9370469409",
      "metadata": {
        "id": "4370a607-ad43-4c5d-bddd-1a9370469409"
      },
      "source": [
        "***\n",
        "\n",
        "## 3.1. *Pipeline* de preprocesamiento\n",
        "\n",
        "Se construye el conjunto **`english_stop`** a partir de las *stop-words* inglesas de **`NLTK`** y se define la funci√≥n **`clean_and_tokenize`** [¬π]:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stop = set(stopwords.words('english'))\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    # extrae solo palabras alfab√©ticas\n",
        "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
        "    return [t for t in tokens if t not in english_stop]\n",
        "\n",
        "# Prueba\n",
        "example = data.loc[0, 'text']\n",
        "print(clean_and_tokenize(example))"
      ],
      "metadata": {
        "id": "3mx_OdLIDyhG"
      },
      "id": "3mx_OdLIDyhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se construye el vocabulario y define la funci√≥n de codificaci√≥n:\n",
        "\n",
        "1. **`all_tokens`** concatena todos los tokens de las rese√±as ya limpiadas.  \n",
        "2. Con **`Counter`** se calcula **`freq`**, el conteo de frecuencia de cada token.  \n",
        "3. Se crea **`vocab`** guardando las 20 000 palabras m√°s frecuentes y asignando\n",
        "   √≠ndices a partir del 2; los √≠ndices **`0`** y **`1`** quedan reservados para los\n",
        "   s√≠mbolos especiales **`<PAD>`** y **`<UNK>`** que se a√±aden despu√©s.\n",
        "4. **`encode(tokens, max_len=200)`** convierte una lista de tokens en su\n",
        "   representaci√≥n num√©rica mediante el diccionario **`vocab`**.  \n",
        "   - Cada palabra se mapea a su √≠ndice; las fuera de vocabulario usan **`1`**  \n",
        "     (**`<UNK>`**).  \n",
        "   - La secuencia resultante se recorta a **`max_len`** y se rellena con **`0`**\n",
        "     (**`<PAD>`**) hasta alcanzar exactamente esa longitud.\n"
      ],
      "metadata": {
        "id": "TFInYXoOXHTh"
      },
      "id": "TFInYXoOXHTh"
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = [tok for txt in data['text'] for tok in clean_and_tokenize(txt)]\n",
        "freq = Counter(all_tokens)\n",
        "\n",
        "# Top‚Äë20‚ÄØ000 + <PAD>=0, <UNK>=1\n",
        "vocab = {w:i+2 for i,(w,_) in enumerate(freq.most_common(20000))}\n",
        "#Indicadores de inicio y final\n",
        "vocab.update({'<PAD>':0,'<UNK>':1})\n",
        "\n",
        "def encode(tokens, max_len=200):\n",
        "    seq = [vocab.get(t,1) for t in tokens]\n",
        "    # Completa secuencia hasta tener max_len\n",
        "    return seq[:max_len] + [0]*(max_len-len(seq))"
      ],
      "metadata": {
        "id": "zfp-stYtEt71"
      },
      "id": "zfp-stYtEt71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funci√≥n **`build_embedding_matrix`** genera la matriz de pesos que inicializar√° la capa **`Embedding`** del modelo:"
      ],
      "metadata": {
        "id": "dFNdGo_cXGTs"
      },
      "id": "dFNdGo_cXGTs"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(vocab, pretrained, emb_dim, unk_init=\"random\"):\n",
        "    matrix = np.zeros((len(vocab), emb_dim), dtype=np.float32)\n",
        "    for word, idx in vocab.items():\n",
        "        if word in pretrained:\n",
        "            matrix[idx] = pretrained[word]\n",
        "        elif hasattr(pretrained, 'key_to_index') and word in pretrained.key_to_index:\n",
        "            # caso Gensim KeyedVectors\n",
        "            matrix[idx] = pretrained[word]\n",
        "        else:\n",
        "            if unk_init==\"random\":\n",
        "                matrix[idx] = np.random.normal(scale=0.6, size=(emb_dim,))\n",
        "            # de lo contrario deja ceros\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "nZt2seoiKfRy"
      },
      "id": "nZt2seoiKfRy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se construye la matriz **`w2v_weights`** aplicando **`build_embedding_matrix`** al vocabulario y al modelo *Word2Vec GoogleNews*. Este modelo contiene 3 millones de vectores de 300 dimensiones entrenados con el algoritmo skip-gram con negative sampling sobre ‚âà 100 mil millones de palabras del corpus Google News. Los autores sustituyeron las redes n-gram tradicionales por dos arquitecturas sencillas (CBOW y Skip-gram) con optimizaciones como negative sampling y sub-sampling de palabras frecuentes. Esto permiti√≥ entrenar embeddings de alta calidad en horas, no en d√≠as [¬≤].\n",
        "\n",
        "Esto vectores se cracter√≠za por capturar relaciones aditivas (‚Äúking ‚àí man + woman ‚âà queen‚Äù) y mejoraron sustancialmente las tareas de analog√≠as y similitud frente a m√©todos previos. ESu amplio vocabulario y la granularidad de 300 dimensiones hacen que todav√≠a sea un punto de partida competitivo para tareas de PLN en ingl√©s, sobre todo cuando el corpus propio es peque√±o o se busca ahorrar tiempo de entrenamiento."
      ],
      "metadata": {
        "id": "MFBM8AiDXFqo"
      },
      "id": "MFBM8AiDXFqo"
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_weights = build_embedding_matrix(vocab, w2v_model, emb_dim=300)\n",
        "# glove_weights = build_embedding_matrix(vocab, glove_model, emb_dim=100)"
      ],
      "metadata": {
        "id": "bTgwSPGsKgzI"
      },
      "id": "bTgwSPGsKgzI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nube de palabras"
      ],
      "metadata": {
        "id": "IOYz0UWa9jRl"
      },
      "id": "IOYz0UWa9jRl"
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_wordcloud(X: pd.DataFrame):\n",
        "\n",
        "  # Foo\n",
        "  X_preprocesado = X.text.apply(lambda x: clean_and_tokenize(x))\n",
        "  texto_nube = ' '.join(w for text in X_preprocesado for w in text)\n",
        "\n",
        "  # Generaci√≥n Nube de Palabras\n",
        "  wordcloud = WordCloud(\n",
        "      width=800,\n",
        "      height=400,\n",
        "      background_color ='white',\n",
        "      min_font_size=10,\n",
        "      max_font_size=110,\n",
        "      max_words=100\n",
        "  ).generate(texto_nube)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "sBwcvN1xtktk"
      },
      "id": "sBwcvN1xtktk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time generar_wordcloud(data)"
      ],
      "metadata": {
        "id": "3Rof6FeuAQNF"
      },
      "id": "3Rof6FeuAQNF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "G6-RrjwuxYEw",
      "metadata": {
        "id": "G6-RrjwuxYEw"
      },
      "source": [
        "***\n",
        "\n",
        "# 4. Desarrollo del modelo RNN...."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 4.1. Hiperpar√°metros, partici√≥n y *DataLoaders*\n",
        "\n",
        "Se crea un **`IMDBDataset`** que toma cada rese√±a del **`DataFrame`**, la limpia, la tokeniza, la codifica a 200 √≠ndices con **`padding`** y entrega la pareja tensor entero + etiqueta flotante , lista para **`nn.BCELoss`**. Con una semilla fija se divide el conjunto en **`80 %‚Äì10 %‚Äì10 %`** (entrenamiento, validaci√≥n y prueba) y cada segmento se coloca en un **`DataLoader`** de lote 64, barajado solo en entrenamiento."
      ],
      "metadata": {
        "id": "wqy-xlwJ2zMS"
      },
      "id": "wqy-xlwJ2zMS"
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, df, max_len=200):\n",
        "        self.texts  = df['text'].tolist()\n",
        "        self.labels = df['label'].astype(int).tolist()\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        toks = clean_and_tokenize(self.texts[i])\n",
        "        return torch.tensor(encode(toks,self.max_len)), torch.tensor(self.labels[i],dtype=torch.float)\n",
        "\n",
        "# Split 80/10/10 con semilla fija\n",
        "ds = IMDBDataset(data)\n",
        "n = len(ds)\n",
        "train_ds, val_ds, test_ds = random_split(ds, [int(.8*n),int(.1*n),n-int(.9*n)], generator=torch.Generator().manual_seed(RANDOM_STATE))\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "OIoaU3iKH6ey"
      },
      "id": "OIoaU3iKH6ey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`SentimentRNN`**, derivada de **`Module`**, integra cuatro componentes clave en un √∫nico flujo [¬≥]. Primero aparece la capa de *embeddings*: si se suministran **`pretrained_weights`**, estos vectores se cargan mediante **`Embedding.from_pretrained`** y pueden mantenerse fijos o descongelarse seg√∫n convenga; de lo contrario, se crea un **`Embedding`** est√°ndar con el tama√±o del vocabulario. A continuaci√≥n se encuentra la LSTM, configurable en dimensi√≥n oculta, n√∫mero de capas y bidireccionalidad. Cuando se activa el modo bidireccional, el modelo recorre cada secuencia tanto en su orden natural como en sentido inverso, capturando dependencias de corto y largo alcance, y aplica *dropout* interno entre capas para mejorar la generalizaci√≥n.\n",
        "\n",
        "Tras procesar la secuencia, se conserva √∫nicamente el √∫ltimo estado temporal ‚Äîque ya sintetiza la informaci√≥n m√°s relevante‚Äî y se le aplica un **`Dropout`** externo como medida adicional contra el sobre‚Äëajuste. Finalmente, una capa totalmente conectada reduce la representaci√≥n a un valor escalar que **`sigmoid`** transforma en la probabilidad de que la rese√±a sea positiva. El m√©todo **`forward`** encadena estos pasos y devuelve el resultado aplanado con **`.squeeze`** para facilitar la comparaci√≥n con la etiqueta.\n",
        "\n",
        "---\n",
        "\n",
        "La preferencia por LSTM sobre alternativas como GRU responde a tres factores complementarios. Primero, las rese√±as de IMDb rondan los 200 *tokens*, por lo que las dependencias de largo plazo resultan importantes y la memoria expl√≠cita de la LSTM ofrece una ventaja. Segundo, el tama√±o del corpus es lo bastante grande como para entrenar un modelo con m√°s par√°metros sin incurrir en sobre‚Äëajuste excesivo, especialmente con las t√©cnicas de *dropout* ya mencionadas. Tercero y decisivo, un estudio reciente muestra que en este mismo conjunto de datos la LSTM supera consistentemente a la GRU en *accuracy*, corroborando hallazgos anteriores [‚Å¥]. A la luz de esta evidencia y de los requisitos de desempe√±o del proyecto, la LSTM se presenta como la opci√≥n m√°s s√≥lida."
      ],
      "metadata": {
        "id": "vPRMS7KNUVmE"
      },
      "id": "vPRMS7KNUVmE"
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=100, hid_dim=128, n_layers=2, bidir=True, drop=0.6, pretrained_weights=None):\n",
        "        super().__init__()\n",
        "        #Capa de embedding\n",
        "        if pretrained_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.FloatTensor(pretrained_weights),\n",
        "                freeze=False,\n",
        "                padding_idx=0\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        # RNN\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional=bidir, batch_first=True, dropout=drop)\n",
        "        # Droput\n",
        "        self.dropout = nn.Dropout(drop)\n",
        "        # Red completamente conectada\n",
        "        self.fc = nn.Linear(hid_dim*(2 if bidir else 1), 1)\n",
        "    def forward(self, x):\n",
        "        emb, _ = self.embedding(x), None\n",
        "        out, _ = self.lstm(emb)\n",
        "        h_last = out[:, -1, :]\n",
        "        return torch.sigmoid(self.fc(self.dropout(h_last))).squeeze()"
      ],
      "metadata": {
        "id": "cipLcGcoKLdm"
      },
      "id": "cipLcGcoKLdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 4.3. Entrenamiento, validaci√≥n y prueba\n",
        "\n",
        "La funci√≥n **`train`** recorre las √©pocas alternando entrenamiento y validaci√≥n, calcula p√©rdidas y precisiones, y registra todo en **`history`**. Antes de cada actualizaci√≥n aplica **`gradient clipping`** mediante **`nn.utils.clip_grad_norm_`**, limitando la norma de los gradientes a **`5.0`**; esto previene el problema de **`exploding gradients`**, que puede desestabilizar el aprendizaje al producir actualizaciones desmesuradas en los pesos, sobre todo en redes recurrentes y arquitecturas profundas [‚Åµ]. Tras cada √©poca se guarda el mejor modelo seg√∫n la p√©rdida de validaci√≥n y se activa **`Early Stopping`** si no hay mejora durante el n√∫mero de √©pocas definido por **`patience`**; al finalizar, la funci√≥n devuelve **`history`** para an√°lisis posterior.\n"
      ],
      "metadata": {
        "id": "oz4ZPXG44a8W"
      },
      "id": "oz4ZPXG44a8W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrena una red para clasificaci√≥n binaria con Early Stopping y gradient clipping.\n",
        "def train( model,train_loader,val_loader,epochs=10,lr=1e-3,device=\"cuda\",clip=5.0,patience=3, save_path=\"best_model.pt\"):\n",
        "\n",
        "    criterion  = nn.BCELoss()\n",
        "    optimizer  = optim.Adam(model.parameters(), lr=lr)  # <‚Äî usa el lr pasado\n",
        "    best_val   = float(\"inf\")\n",
        "    epochs_no_imp = 0\n",
        "\n",
        "    history = {k: [] for k in (\"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\")}\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # ---------- Fase de entrenamiento ----------\n",
        "        model.train()\n",
        "        running_loss, running_corrects, total_train = 0.0, 0, 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss    = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)  # gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss     += loss.item() * x.size(0)\n",
        "            preds             = (outputs >= 0.5).float()\n",
        "            running_corrects += (preds == y).sum().item()\n",
        "            total_train      += x.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / total_train\n",
        "        epoch_train_acc  = running_corrects / total_train\n",
        "\n",
        "        # ---------- Fase de validaci√≥n ----------\n",
        "        model.eval()\n",
        "        val_loss, val_corrects, total_val = 0.0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                outputs = model(x)\n",
        "                val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "\n",
        "                preds        = (outputs >= 0.5).float()\n",
        "                val_corrects += (preds == y).sum().item()\n",
        "                total_val    += x.size(0)\n",
        "\n",
        "        epoch_val_loss = val_loss / total_val\n",
        "        epoch_val_acc  = val_corrects / total_val\n",
        "\n",
        "        # ---------- Registro de m√©tricas ----------\n",
        "        history[\"train_loss\"].append(epoch_train_loss)\n",
        "        history[\"val_loss\"].append(epoch_val_loss)\n",
        "        history[\"train_acc\"].append(epoch_train_acc)\n",
        "        history[\"val_acc\"].append(epoch_val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train Loss {epoch_train_loss:.4f}  Acc {epoch_train_acc:.4f} | \"\n",
        "            f\"Val Loss {epoch_val_loss:.4f}  Acc {epoch_val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # ---------- Early Stopping ----------\n",
        "        if epoch_val_loss < best_val:\n",
        "            best_val = epoch_val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            epochs_no_imp = 0\n",
        "        else:\n",
        "            epochs_no_imp += 1\n",
        "            if epochs_no_imp >= patience:\n",
        "                print(\"‚Üí Early stopping\")\n",
        "                break\n",
        "\n",
        "    mins, secs = divmod(time.time() - start_time, 60)\n",
        "    print(f\"\\nTiempo total: {int(mins)} min {int(secs)} seg\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "DVO5O3T1Kh4O"
      },
      "id": "DVO5O3T1Kh4O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se instancia **`model_w2v`** como una versi√≥n de **`SentimentRNN`** cuyos vectores de entrada provienen de un embebido **`Word2Vec`** de 300 dimensiones (**`w2v_weights`**) y se mueve al **`device`** seleccionado (GPU o CPU). Con este modelo se llama a **`train`** durante 15 √©pocas, una tasa de aprendizaje de **`2 √ó 10‚Åª‚Å¥`**, *gradient clipping* y *early stopping* habilitados, utilizando los **`DataLoaders`** de entrenamiento y validaci√≥n. El historial de p√©rdidas y precisiones se almacena en **`hist_w2v`**, mientras que los mejores pesos (seg√∫n la p√©rdida de validaci√≥n m√°s baja) se guardan en **`best_w2v.pt`** para uso posterior."
      ],
      "metadata": {
        "id": "75G3J6o5ieVc"
      },
      "id": "75G3J6o5ieVc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "model_w2v = SentimentRNN(\n",
        "    vocab_size=len(vocab),\n",
        "    emb_dim=300,\n",
        "    pretrained_weights=w2v_weights\n",
        ").to(device)\n",
        "\n",
        "hist_w2v = train(\n",
        "    model_w2v,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=15,\n",
        "    lr=2e-4,\n",
        "    device=device,\n",
        "    save_path=\"best_w2v.pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZYBhqK0KKzNt"
      },
      "id": "ZYBhqK0KKzNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7JgtiyyfIyMM",
      "metadata": {
        "id": "7JgtiyyfIyMM"
      },
      "source": [
        "***\n",
        "\n",
        "\n",
        "# 5. An√°lisis de resultados y conclusiones\n",
        "\n",
        "Se instancia un nuevo **`SentimentRNN`** con los pesos de **`Word2Vec`** (300 dimensiones) y se env√≠a al **`device`** detectado; a continuaci√≥n se cargan los par√°metros previamente entrenados desde **`best_w2v.pt`** mediante **`load_state_dict(torch.load(...))`**, dejando al modelo **`best_w2v`** listo para inferencia o evaluaci√≥n sin requerir m√°s entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_w2v = SentimentRNN(len(vocab), emb_dim=300, pretrained_weights=w2v_weights).to(device)\n",
        "best_w2v.load_state_dict(torch.load(\"best_w2v.pt\"))"
      ],
      "metadata": {
        "id": "MwXrB3ThRHcI"
      },
      "id": "MwXrB3ThRHcI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la funci√≥n **`plot_history`** que recibe el diccionario **`history`** y crea dos gr√°ficos de l√≠nea con **`matplotlib`**: el primero compara **`train_loss`** y **`val_loss`** por √©poca, y el segundo hace lo mismo con **`train_acc`** y **`val_acc`**."
      ],
      "metadata": {
        "id": "Ty3gzWEui8xK"
      },
      "id": "Ty3gzWEui8xK"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # P√©rdidas\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs, history['val_loss'],   label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Exactitudes\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(epochs, history['train_acc'], label='Train Acc')\n",
        "    plt.plot(epochs, history['val_acc'],   label='Val Acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d4PT-Gz1Y9Mh"
      },
      "id": "d4PT-Gz1Y9Mh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(hist_w2v)"
      ],
      "metadata": {
        "id": "hkHiJTa-ZAFC"
      },
      "id": "hkHiJTa-ZAFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El gr√°fico muestra c√≥mo la p√©rdida de entrenamiento (**`Train Loss`**) desciende de manera casi mon√≥tona, mientras que la p√©rdida de validaci√≥n (**`Val Loss`**) baja lentamente hasta la √©poca 5, oscila con un pico pronunciado en la √©poca 9 y alcanza su m√≠nimo en la 10, punto en el que se reduce la brecha entre ambas curvas. A partir de ah√≠ la **`Train Loss`** sigue disminuyendo, pero la **`Val Loss`** se estabiliza y vuelve a subir levemente, se√±al de sobreajuste. El uso de *early stopping* detuvo el entrenamiento cerca del √≥ptimo de validaci√≥n, evitando que el modelo se desv√≠e; sin embargo, la variabilidad previa sugiere que podr√≠an mejorarse la regularizaci√≥n (m√°s **`dropout`** o **`weight decay`**) o la diversidad de datos para lograr curvas de validaci√≥n m√°s suaves y una convergencia m√°s estable."
      ],
      "metadata": {
        "id": "qJZ_NDzdjQ7G"
      },
      "id": "qJZ_NDzdjQ7G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se cargan los pesos √≥ptimos en **`model_w2v`** con **`load_state_dict`** y se cambia a modo evaluaci√≥n mediante **`eval()`** para desactivar **`dropout`** y el c√°lculo de gradientes. A continuaci√≥n se recorre el **`test_loader`** sin seguimiento de gradiente (**`torch.no_grad()`**); cada lote se env√≠a al **`device`**, se obtiene la probabilidad de clase positiva, se aplica un umbral de **`0.5`** para convertirla en etiqueta binaria y se acumulan predicciones (**`y_pred`**) y verdaderos (**`y_true`**). Finalmente se calculan cuatro m√©tricas de rendimiento: **`accuracy_score`** (proporci√≥n de aciertos globales), **`recall_score`** (sensibilidad hacia la clase positiva), **`f1_score`** (promedio arm√≥nico entre precisi√≥n y _recall_) y **`confusion_matrix`** (tabla de errores y aciertos), proporcionando una visi√≥n cuantitativa de la capacidad generalizadora del modelo entrenado."
      ],
      "metadata": {
        "id": "We4fOjKpj4_8"
      },
      "id": "We4fOjKpj4_8"
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.load_state_dict(torch.load('best_w2v.pt'))\n",
        "model_w2v.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        out = (model_w2v(x.to(device)).cpu().numpy() >= 0.5).astype(int)\n",
        "        y_pred.extend(out.tolist())\n",
        "        y_true.extend(y.numpy().astype(int).tolist())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true,y_pred))\n",
        "print(\"Recall:\", recall_score(y_true,y_pred))\n",
        "print(\"F1‚Äëscore:\", f1_score(y_true,y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true,y_pred))"
      ],
      "metadata": {
        "id": "QI42wZnGNDhC"
      },
      "id": "QI42wZnGNDhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm      = confusion_matrix(y_true, y_pred)\n",
        "class_names = ['Negative', 'Positive']\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel(\"Predicci√≥n\")\n",
        "plt.ylabel(\"Etiqueta real\")\n",
        "plt.title(\"Matriz de confusi√≥n ‚Äî Conjunto de prueba\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VY-_TQIJkJbw"
      },
      "id": "VY-_TQIJkJbw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo alcanza una **`accuracy`** del **`0.878`**, lo que significa que casi 88 % de las rese√±as se clasifican correctamente. Con un **`recall`** de **`0.850`**, identifica alrededor del 85 % de las rese√±as realmente positivas; unido a un **`F1-score`** de **`0.875`**, esto indica un buen equilibrio entre precisi√≥n y recuperaci√≥n de la clase positiva.\n",
        "\n",
        "La matriz de confusi√≥n muestra que el modelo confunde m√°s frecuentemente positivas como negativas (301 FN) que al rev√©s (184 FP). Aun as√≠, la proporci√≥n de verdaderos positivos (1700) y verdaderos negativos (1788) domina el total, evidenciando un desempe√±o s√≥lido pero con margen para reducir los **`false negatives`**‚Äîp. ej., ajustando el umbral de decisi√≥n o a√±adiendo regularizaci√≥n para mejorar la cobertura de la clase positiva."
      ],
      "metadata": {
        "id": "ZCYFkXAEkRND"
      },
      "id": "ZCYFkXAEkRND"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funci√≥n **`predict_sentiment`** recibe un texto libre y, sin necesidad de gradientes, lo procesa en cinco pasos: primero lo limpia y tokeniza con **`clean_and_tokenize`**; luego transforma cada token en su √≠ndice mediante el **`vocab`**, rellenando con **`<PAD>`** o truncando hasta **`sequence_length` = 200**. La secuencia resultante se convierte en un tensor de lote 1, se env√≠a al **`device`** (CPU/GPU) y se pasa por el modelo **`SentimentRNN`** en modo evaluaci√≥n, obteniendo una probabilidad entre 0 y 1. Finalmente imprime el **`Score de positividad`** y muestra un mensaje üü¢ o üî¥ seg√∫n si la probabilidad supera el umbral de 0.5, permitiendo clasificar r√°pidamente cualquier rese√±a como positiva o negativa."
      ],
      "metadata": {
        "id": "i7hL2aoJkok-"
      },
      "id": "i7hL2aoJkok-"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(review_text: str,\n",
        "                      model: nn.Module,\n",
        "                      vocab: dict,\n",
        "                      sequence_length: int = 200,\n",
        "                      device: torch.device = device) -> None:\n",
        "    \"\"\"\n",
        "    Usa el modelo entrenado para predecir si `review_text` es positiva o negativa.\n",
        "    - review_text: cadena con la rese√±a a clasificar.\n",
        "    - model: instancia de SentimentRNN ya entrenada.\n",
        "    - vocab: diccionario palabra->√≠ndice.\n",
        "    - sequence_length: longitud fija del input (padding/truncado).\n",
        "    - device: 'cuda' o 'cpu'.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1) Tokenizar y limpiar\n",
        "    tokens = clean_and_tokenize(review_text)\n",
        "\n",
        "    # 2) Codificar + pad/truncado\n",
        "    idxs = [vocab.get(t, vocab['<UNK>']) for t in tokens]\n",
        "    if len(idxs) < sequence_length:\n",
        "        idxs += [vocab['<PAD>']] * (sequence_length - len(idxs))\n",
        "    else:\n",
        "        idxs = idxs[:sequence_length]\n",
        "\n",
        "    # 3) Tensor batch_size=1 y mover a device\n",
        "    x = torch.tensor([idxs], dtype=torch.long).to(device)\n",
        "\n",
        "    # 4) Forward sin gradientes\n",
        "    with torch.no_grad():\n",
        "        prob = model(x).item()  # valor [0,1]\n",
        "\n",
        "    # 5) Mostrar resultados\n",
        "    print(f\"‚û°Ô∏è  Score de positividad: {prob:.4f}\")\n",
        "    if prob >= 0.5:\n",
        "        print(\"üü¢  Positive review detected!\")\n",
        "    else:\n",
        "        print(\"üî¥  Negative review detected!\")\n"
      ],
      "metadata": {
        "id": "tbtQicsqWwTa"
      },
      "id": "tbtQicsqWwTa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"Thanks to this movie we are all dumber than before.\"\n",
        "predict_sentiment(review, model_w2v, vocab)"
      ],
      "metadata": {
        "id": "hs_gr1QGW2D5"
      },
      "id": "hs_gr1QGW2D5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La rese√±a  recibi√≥ un **`score de positividad`** de **`0.215`**, muy por debajo del umbral **`0.5`**, por lo que el modelo la clasifica como **`Negative review`** con alta confianza. La probabilidad refleja que los t√©rminos ¬´dumber¬ª y la construcci√≥n ir√≥nica ¬´Thanks to this movie‚Ä¶¬ª transmiten un juicio claramente desfavorable, pero que un modelo b√°sico podr√≠a confundir al valorar positivamente la palabra *thanks*. El LSTM ha aprendido a asociar esta combinaci√≥n de palabras con la clase negativa. Si se quisiera afinar la detecci√≥n de sarcasmo o matices sutiles, podr√≠a bajarse el umbral o incorporar ejemplos etiquetados de lenguaje sarc√°stico para mejorar la sensibilidad en este tipo de frases."
      ],
      "metadata": {
        "id": "ONaHERJlkywv"
      },
      "id": "ONaHERJlkywv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "\n",
        "# 6. Conclusi√≥n\n",
        "\n",
        "El clasificador basado en **`SentimentRNN + Word2Vec`** alcanza un rendimiento s√≥lido (‚âà 88 % de *accuracy* y F1 ‚âà 0.88), aunque a√∫n comete m√°s falsos negativos que falsos positivos. La prueba con la rese√±a ‚ÄúThanks to this movie we are all dumber than before.‚Äù confirma que el modelo detecta cr√≠ticas evidentes. Para mejorar, convendr√≠a (i) ajustar el umbral o ponderar la clase positiva para reducir falsos negativos, (ii) reforzar la regularizaci√≥n (*dropout*, *weight decay*) y explorar b√∫squeda de hiperpar√°metros, y (iii) ampliar el repertorio de ejemplos ‚Äîincluido sarcasmo‚Äî o probar representaciones m√°s ricas (p. ej., embeddings contextualizados o atenci√≥n) que capten mejor los matices ling√º√≠sticos."
      ],
      "metadata": {
        "id": "uDjo26cIk10k"
      },
      "id": "uDjo26cIk10k"
    },
    {
      "cell_type": "markdown",
      "id": "DwUWQIAE3O0o",
      "metadata": {
        "id": "DwUWQIAE3O0o"
      },
      "source": [
        "***\n",
        "\n",
        "\n",
        "# 7. Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BoLDjBS03xiQ",
      "metadata": {
        "id": "BoLDjBS03xiQ"
      },
      "source": [
        "\n",
        "[¬π] **Pytorch, Sentiment Analysis**  \n",
        "Disponible en: [github.com](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/2%20-%20Recurrent%20Neural%20Networks.ipynb)\n",
        "\n",
        "[¬π] **GoogleNews-vectors-negative300**  \n",
        "Disponible en: [kaggle.com](https://www.kaggle.com/datasets/adarshsng/googlenewsvectors)\n",
        "\n",
        "\n",
        "[¬≥] **Sentiment Analysis with an RNN**  \n",
        "Disponible en: [github.com](https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=TJHNs4FZpmwj)\n",
        "\n",
        "\n",
        "\n",
        "[‚Å¥] **LSTM and GRU Neural Network Performance Comparison Study: Taking Yelp Review Dataset as an Example**  \n",
        "Disponible en: [researchgate.net/](https://www.researchgate.net/publication/347267378_LSTM_and_GRU_Neural_Network_Performance_Comparison_Study_Taking_Yelp_Review_Dataset_as_an_Example)\n",
        "\n",
        "\n",
        "[‚Åµ] **Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)**  \n",
        "Disponible en: [neptune.ai](https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem?utm_source=chatgpt.com)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}