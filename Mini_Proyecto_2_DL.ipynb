{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorbnc/MAIA-DL/blob/master/Mini_Proyecto_2_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oa_MJEGQ6jTi",
      "metadata": {
        "id": "oa_MJEGQ6jTi"
      },
      "source": [
        "![Universidad_de_los_Andes_30.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sl89pic17iPU",
      "metadata": {
        "id": "sl89pic17iPU"
      },
      "source": [
        "<style>\n",
        "@import url('https://fonts.googleapis.com/css2?family=Latin+Modern+Roman:wght@400;700&display=swap');\n",
        "\n",
        "body, p, h1, h2, h3, h4, h5, h6, li {\n",
        "  font-family: 'Latin Modern Roman', serif;\n",
        "}\n",
        "code, pre {\n",
        "  font-family: 'Fira Mono', monospace;\n",
        "}\n",
        "</style>\n",
        "\n",
        "***\n",
        "\n",
        "# **Mini Proyecto 2, Técnicas de *Deep Learning*: Clasificación de Sentimientos de Reseñas de Películas en *IMDB* con Redes Neuronales Recurrentes**\n",
        "\n",
        "## **Descripción del problema:**\n",
        "\n",
        "En este proyecto abordamos el problema de clasificación de sentimiento en reseñas de películas. Partimos de un conjunto de datos extraído de Kaggle que contiene reseñas textuales (**`text`**) y una etiqueta binaria (**`label`**) que indica si la opinión es positiva (**1**) o negativa (**0**). El reto consiste en diseñar un modelo de *Deep Learning*, concretamente una Red Neuronal Recurrente con capas LSTM, capaz de procesar secuencias de texto y predecir con alta precisión la polaridad de cada reseña.\n",
        "\n",
        "## **Objetivo:**\n",
        "\n",
        "* Implementar un *pipeline* completo que incluya la descarga del dataset, el preprocesamiento de texto (tokenización, limpieza, construcción de vocabulario y padding de secuencias), y la definición de un Dataset y DataLoader en PyTorch.\n",
        "\n",
        "* Diseñar una arquitectura basada en *embeddings* y una o varias capas LSTM (bidireccionales), con regularización por *dropout* y optimización con **`Adam`**.\n",
        "\n",
        "* Entrenar el modelo utilizando GPU cuando esté disponible, incorporando técnicas de *early stopping* y gradient clipping para evitar sobreajuste y explosión de gradientes.\n",
        "\n",
        "* Evaluar su desempeño final sobre el conjunto de prueba, calculando *accuracy*, *F1‑score*, *recall* y presentando la matriz de confusión.\n",
        "\n",
        "* Visualizar la evolución de la pérdida y la exactitud en entrenamiento y validación a lo largo de las épocas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SiCS5j-HAbCB",
      "metadata": {
        "id": "SiCS5j-HAbCB"
      },
      "source": [
        "***\n",
        "\n",
        "**Este proyecto es realizado por Andrés Felipe Ñungo y Jordan Bryan Núñez Campos para entrega el 9 de mayo.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# Tareas\n",
        "\n",
        "* Exploración y pre-procesamiento (Jordan)\n",
        "\n",
        "* Introducción, y explicación de las opciones tomadas en el PDF (Andrés y Jordan)\n",
        "\n",
        "* Comentarios en código, explicaciones en markdowns (Andrés y Jordan)\n",
        "\n",
        "* Nube de Palabras (Jordan) ✅\n",
        "\n",
        "* Validación de idiomas (Jordan) ✅\n",
        "\n",
        "* Glove  (Jordan)\n",
        "\n",
        "* Word2vec (Andrés) ✅\n",
        "\n",
        "* LSTM o GRU  (Andrés) ✅\n",
        "\n",
        "* Mejorar red (Andrés) ✅\n",
        "\n",
        "* Pensar en ideas visualización (Andrés y Jordan)\n",
        "- t-SNE\n",
        "\n",
        "* Entrenamiento y evaluación de modelo (Andrés) ✅\n",
        "\n",
        "* Informe: Arquitectura, entrenamiento, evaluación, análisis cuantitativo (Andrés)\n",
        "\n",
        "* Informe: Exploración, pre-procesamiento, análisis cualitativo, discusión (Jordan)"
      ],
      "metadata": {
        "id": "aP4T8CvEqYMG"
      },
      "id": "aP4T8CvEqYMG"
    },
    {
      "cell_type": "markdown",
      "id": "8VqTMx7TYgjH",
      "metadata": {
        "id": "8VqTMx7TYgjH"
      },
      "source": [
        "\n",
        "***\n",
        "# **Índice**\n",
        "\n",
        "El *notebook* abordará el proyecto de la siguiente manera:\n",
        "\n",
        "| 🔹 | Sección        |\n",
        "|----|----------------|\n",
        "| 1️⃣ | **Instalación y carga de librerías** |\n",
        "| 2️⃣ | **Análisis exploratorio y preparación de los datos**       |\n",
        "| 3️⃣ | **Definición de *pipelines* de procesamiento**          |\n",
        "| 3️⃣.1️⃣ | **Pipeline de preprocesamiento ...**   |\n",
        "| 4️⃣ | **Desarrollo del modelo RNN...**   |\n",
        "| 4️⃣.1️⃣ | **Hiperparámetros, partición y Dataloaders**   |\n",
        "| 4️⃣.2️⃣ | **Adaptación a partir de la arquitectura ...*   |\n",
        "| 4️⃣.3️⃣ | **Entrenamiento, validación y prueba**   |\n",
        "| 5️⃣ | **Análisis de resultados y conclusiones**   |\n",
        "| 6️⃣ | **Conclusión**   |\n",
        "| 7️⃣ | **Referencias**   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cyVuwlHB_W3",
      "metadata": {
        "id": "2cyVuwlHB_W3"
      },
      "source": [
        "***\n",
        "\n",
        "# 1. Instalación y cargue de librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de la instalación de algunas librerías se importan utilidades generales (**`os`**, **`random`**,**`time`**, **`NumPy`**, **`pandas`**), herramientas de PLN con **`NLTK`** (incluida la descarga de *stop-words*), módulos de **`PyTorch`** para construir y entrenar la red, métricas de **`scikit-learn`** y utilidades de visualización con **`Matplotlib`** y **`Seaborn`**. Por último, importa la librería **`huggingface_hub`** para más adelante descargar uno de los embeddings."
      ],
      "metadata": {
        "id": "DkjBiBB5rU6t"
      },
      "id": "DkjBiBB5rU6t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librerías para colab\n",
        "%pip install -q --no-cache-dir kagglehub langdetect\n",
        "%pip install -q --no-cache-dir huggingface_hub"
      ],
      "metadata": {
        "id": "l26426jMLCxE"
      },
      "id": "l26426jMLCxE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerías comunes\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Descarga de datasets y de embeddings\n",
        "import kagglehub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Preprocesamiento, NLP\n",
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "# Palabras vacías\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Modelado\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluación\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Librerías para visualizaciones\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "osjmTJMvtxua"
      },
      "id": "osjmTJMvtxua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importa **`kagglehub`** y ejecuta **`dataset_download`** con el identificador **`yasserh/imdb-movie-ratings-sentiment-analysis`** para descargar el conjunto de datos de reseñas de películas IMDB. La función guarda los archivos de manera local y devuelve la ruta absoluta, que se almacena en **`path`** y se muestra en pantalla mediante **`print`** para confirmar dónde quedaron los datos."
      ],
      "metadata": {
        "id": "yHwealvATakn"
      },
      "id": "yHwealvATakn"
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"yasserh/imdb-movie-ratings-sentiment-analysis\")\n",
        "print(\"Datos descargados en:\", path)"
      ],
      "metadata": {
        "id": "F4y-tzhMl2CC"
      },
      "id": "F4y-tzhMl2CC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se carga los *embeddings* de los *tokens* de los documentos, que nos permitirán tener un nuevo espacio de representación más enriquecido. Más adelante se explica con más detalle cada tipo de *embedding*.\n"
      ],
      "metadata": {
        "id": "07ZBWlW6BudL"
      },
      "id": "07ZBWlW6BudL"
    },
    {
      "cell_type": "code",
      "source": [
        "bin_path = hf_hub_download(repo_id =\n",
        "            \"NathaNn1111/word2vec-google-news-negative-300-bin\",\n",
        "            filename = \"GoogleNews-vectors-negative300.bin\")\n",
        "print(\"Embeddings en:\", bin_path)"
      ],
      "metadata": {
        "id": "oxSwr79ozCBB"
      },
      "id": "oxSwr79ozCBB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P . http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip -d ."
      ],
      "metadata": {
        "id": "zZiJq8VNJIhF"
      },
      "id": "zZiJq8VNJIhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = {}\n",
        "\n",
        "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        word = parts[0]\n",
        "        vector = [float(x) for x in parts[1:]]\n",
        "        embeddings[word] = vector\n",
        "print(f\"Loaded {len(embeddings)} word vectors.\")"
      ],
      "metadata": {
        "id": "bUCjN9BVJS6u"
      },
      "id": "bUCjN9BVJS6u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque detecta si el entorno dispone de GPU y selecciona el **`device`** apropiado para PyTorch.  \n",
        "Primero se llama a **`torch.cuda.is_available()`**, que devuelve *True* si se ha asignado una GPU CUDA al runtime de Colab. Según el resultado se imprime un mensaje informativo (“Entrenando en GPU.” o “GPU no disponible, entrenando en CPU.”).  \n",
        "\n",
        "A continuación se construye el objeto **`device`** con **`torch.device(\"cuda\" if train_on_gpu else \"cpu\")`**, que será pasado a la red y a los tensores de entrada para que se ubiquen en la GPU cuando sea posible. Por último se muestra en pantalla el dispositivo elegido."
      ],
      "metadata": {
        "id": "uOceAejmVjqF"
      },
      "id": "uOceAejmVjqF"
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print(\"Entrenando en GPU.\")\n",
        "else:\n",
        "    print(\"GPU no disponible, entrenando en CPU.\")\n",
        "\n",
        "# 2) Elegir device según disponibilidad\n",
        "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")\n",
        "print(f\"   Usando device: {device}\\n\")"
      ],
      "metadata": {
        "id": "LksJ95qEPWVr"
      },
      "id": "LksJ95qEPWVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque hace dos tareas sencillas de soporte:\n",
        "\n",
        "1. Llama a **`warnings.filterwarnings(\"ignore\")`** para ocultar avisos y mantener la salida del notebook limpia.\n",
        "2. Utiliza **`importlib.metadata.version`** para imprimir las versiones de un conjunto de librerías clave ( **`numpy`**, **`pandas`**, **`torch`**, **`torchvision`**, **`scikit-learn`**, **`kagglehub`**, **`pillow`**, **`matplotlib`**, **`seaborn`** ). Mostrar estas versiones al inicio del notebook facilita la reproducibilidad y ayuda a depurar posibles conflictos de dependencias."
      ],
      "metadata": {
        "id": "pfXxKk09Vulp"
      },
      "id": "pfXxKk09Vulp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LSwlUgjUjtR_",
      "metadata": {
        "id": "LSwlUgjUjtR_"
      },
      "outputs": [],
      "source": [
        "# Ignorar las warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Versiones utilizadas\n",
        "from importlib.metadata import version\n",
        "librerias = [\n",
        "    \"numpy\", \"pandas\", \"torch\", \"torchvision\", \"scikit-learn\", \"kagglehub\",\n",
        "    \"pillow\", \"matplotlib\", \"seaborn\",\n",
        "    \"nltk\", \"tqdm\", \"scipy\"\n",
        "]\n",
        "for library in librerias:\n",
        "  print(library, \": \", version(library))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente bloque de código nos permite hacer determininísticas las funciones traídas de **`pytorch`**, y en general controlar todos los pseudo-aleatorios del *notebook*"
      ],
      "metadata": {
        "id": "ok4F_5E1njTi"
      },
      "id": "ok4F_5E1njTi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición del random state y seeds\n",
        "RANDOM_STATE = 13\n",
        "random.seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "metadata": {
        "id": "xdgcOt7gznvJ"
      },
      "id": "xdgcOt7gznvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qK311OLFzPep",
      "metadata": {
        "id": "qK311OLFzPep"
      },
      "source": [
        "***\n",
        "\n",
        "# 2. Análisis exploratorio y preparación de los datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 2.1. Carga y estadísticas generales"
      ],
      "metadata": {
        "id": "4KxKy7H1eID-"
      },
      "id": "4KxKy7H1eID-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4yOzyWKJbov",
      "metadata": {
        "id": "K4yOzyWKJbov"
      },
      "outputs": [],
      "source": [
        "# Descargar el conjunto de datos y almacenar el path en una variable\n",
        "data_raw = pd.read_csv(os.path.join(path, 'movie.csv'))\n",
        "data_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2index = {'Negativa':0, 'Positiva':1}\n",
        "index2label = {0:'Negativa', 1:'Positiva'}"
      ],
      "metadata": {
        "id": "tK_D4XVsnk6c"
      },
      "id": "tK_D4XVsnk6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frecuencias absoluta y relativa de cada clase\n",
        "pd.DataFrame({\n",
        "    \"Review\": index2label,\n",
        "    \"F. Absoluta\": data_raw['label'].value_counts().sort_index(),\n",
        "    \"F. Relativa\": data_raw['label'].value_counts(normalize=True).sort_index(),\n",
        "})"
      ],
      "metadata": {
        "id": "EkiUxk8il_JG"
      },
      "id": "EkiUxk8il_JG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "CNfurSnbmf79",
      "metadata": {
        "id": "CNfurSnbmf79"
      },
      "source": [
        "***\n",
        "\n",
        "## 2.2. Limpieza de los datos\n",
        "\n",
        "En estas sección identificamos y corregimos:\n",
        "\n",
        "* Valores faltantes\n",
        "* Textos duplicados\n",
        "* Textos en otros idiomas distintos al inglés"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw.isna().sum()"
      ],
      "metadata": {
        "id": "2r0P2t_Ysjbq"
      },
      "id": "2r0P2t_Ysjbq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw.duplicated().sum()"
      ],
      "metadata": {
        "id": "m3IgmRbWskuE"
      },
      "id": "m3IgmRbWskuE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_raw.drop_duplicates(keep='first')\n",
        "\n",
        "# Frecuencias absoluta y relativa de cada clase\n",
        "pd.DataFrame({\n",
        "    \"Review\": index2label,\n",
        "    \"F. Absoluta\": data['label'].value_counts().sort_index(),\n",
        "    \"F. Relativa\": data['label'].value_counts(normalize=True).sort_index(),\n",
        "})"
      ],
      "metadata": {
        "id": "37qzkcBL3D3P"
      },
      "id": "37qzkcBL3D3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detectar_otros_idiomas(X: pd.DataFrame):\n",
        "  #Identificar los idiomas del dataframe y los índices de las filas que no están en inglés\n",
        "  X['idioma'] = X['text'].apply(lambda x: detect(x) if isinstance(x, str) and x.strip() else \"desconocido\")\n",
        "  indices_a_traducir = X[X['idioma'] != 'en'].index\n",
        "\n",
        "  # Imprimir textos en otros idiomas\n",
        "  print(\"Textos en otros idiomas:\")\n",
        "  for i in indices_a_traducir:\n",
        "    print(X.loc[i].text)\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "_WQ5tiJps-bO"
      },
      "id": "_WQ5tiJps-bO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar columna 'idioma' al df + ver textos en otros idiomas\n",
        "%time data = detectar_otros_idiomas(data)\n",
        "data.idioma.value_counts()"
      ],
      "metadata": {
        "id": "17fC-8gv1YCD"
      },
      "id": "17fC-8gv1YCD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de esta sección es incorporar varios estilos de pre-procesamiento, probar diferentes maneras de depurar los textos como por ejemplo remover caracteres no alfanuméricos, a excepción de guiones o apóstrofes. También haremos uso de dos normalizadores: *Stemming* y *Lemmatization*, ...."
      ],
      "metadata": {
        "id": "vCm-sqa9t8D2"
      },
      "id": "vCm-sqa9t8D2"
    },
    {
      "cell_type": "markdown",
      "id": "_ypyorGqwmRl",
      "metadata": {
        "id": "_ypyorGqwmRl"
      },
      "source": [
        "***\n",
        "\n",
        "# 3. Definición de *pipelines* de procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4370a607-ad43-4c5d-bddd-1a9370469409",
      "metadata": {
        "id": "4370a607-ad43-4c5d-bddd-1a9370469409"
      },
      "source": [
        "***\n",
        "\n",
        "## 3.1. *Pipeline* de preprocesamiento\n",
        "\n",
        "Se construye el conjunto **`english_stop`** a partir de las *stop-words* inglesas de **`NLTK`** y se define la función **`clean_and_tokenize`** [¹]:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stop = set(stopwords.words('english'))\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    # extrae solo palabras alfabéticas\n",
        "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
        "    return [t for t in tokens if t not in english_stop]\n",
        "\n",
        "# Prueba\n",
        "example = data.loc[0, 'text']\n",
        "print(clean_and_tokenize(example))"
      ],
      "metadata": {
        "id": "3mx_OdLIDyhG"
      },
      "id": "3mx_OdLIDyhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se construye el vocabulario y define la función de codificación:\n",
        "\n",
        "1. **`all_tokens`** concatena todos los tokens de las reseñas ya limpiadas.  \n",
        "2. Con **`Counter`** se calcula **`freq`**, el conteo de frecuencia de cada token.  \n",
        "3. Se crea **`vocab`** guardando las 20 000 palabras más frecuentes y asignando\n",
        "   índices a partir del 2; los índices **`0`** y **`1`** quedan reservados para los\n",
        "   símbolos especiales **`<PAD>`** y **`<UNK>`** que se añaden después.\n",
        "4. **`encode(tokens, max_len=200)`** convierte una lista de tokens en su\n",
        "   representación numérica mediante el diccionario **`vocab`**.  \n",
        "   - Cada palabra se mapea a su índice; las fuera de vocabulario usan **`1`**  \n",
        "     (**`<UNK>`**).  \n",
        "   - La secuencia resultante se recorta a **`max_len`** y se rellena con **`0`**\n",
        "     (**`<PAD>`**) hasta alcanzar exactamente esa longitud.\n"
      ],
      "metadata": {
        "id": "TFInYXoOXHTh"
      },
      "id": "TFInYXoOXHTh"
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = [tok for txt in data['text'] for tok in clean_and_tokenize(txt)]\n",
        "freq = Counter(all_tokens)\n",
        "\n",
        "# Top‑20 000 + <PAD>=0, <UNK>=1\n",
        "vocab = {w:i+2 for i,(w,_) in enumerate(freq.most_common(20000))}\n",
        "#Indicadores de inicio y final\n",
        "vocab.update({'<PAD>':0,'<UNK>':1})\n",
        "\n",
        "def encode(tokens, max_len=200):\n",
        "    seq = [vocab.get(t,1) for t in tokens]\n",
        "    # Completa secuencia hasta tener max_len\n",
        "    return seq[:max_len] + [0]*(max_len-len(seq))"
      ],
      "metadata": {
        "id": "zfp-stYtEt71"
      },
      "id": "zfp-stYtEt71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función **`load_google_vectors_bin`** genera la matriz de pesos que inicializará la capa **`Embedding`** del modelo:"
      ],
      "metadata": {
        "id": "dFNdGo_cXGTs"
      },
      "id": "dFNdGo_cXGTs"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Carga Word2Vec\n",
        "def load_google_vectors_bin(bin_file: str,\n",
        "                            vocab: dict[str, int],\n",
        "                            emb_dim: int = 300) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Lee el fichero Word2Vec binario de Google News y devuelve la matriz\n",
        "    de pesos (|vocab| × emb_dim) lista para nn.Embedding.\n",
        "    Solo extrae los vectores de las palabras presentes en `vocab`;\n",
        "    el resto se inicializa aleatoriamente (N(0, 0.6²)).\n",
        "    \"\"\"\n",
        "    matrix = np.random.normal(scale=0.6,\n",
        "                              size=(len(vocab), emb_dim)\n",
        "                             ).astype(np.float32)\n",
        "\n",
        "    with open(bin_file, \"rb\") as f:\n",
        "        # Cabecera:  \"3000000 300\\n\"\n",
        "        header = f.readline()\n",
        "        total_words, dim = map(int, header.split())\n",
        "        assert dim == emb_dim, \"Dimensión de embedding inesperada\"\n",
        "\n",
        "        binary_len = emb_dim * 4  # 4 bytes • float32\n",
        "        for _ in range(total_words):\n",
        "            # 1) Leer palabra (bytes hasta el primer espacio)\n",
        "            word_bytes = []\n",
        "            while True:\n",
        "                ch = f.read(1)\n",
        "                if ch == b' ':          # separador palabra-vector\n",
        "                    word = b\"\".join(word_bytes).decode(\"utf-8\", \"ignore\")\n",
        "                    break\n",
        "                if ch != b'\\n':         # el \\n solo aparece en la cabecera\n",
        "                    word_bytes.append(ch)\n",
        "\n",
        "            # 2) Leer los 300 float32 (secuencia de 1200 bytes)\n",
        "            vec = np.frombuffer(f.read(binary_len), dtype=np.float32)\n",
        "\n",
        "            # 3) Copiar si la palabra está en nuestro vocabulario\n",
        "            if word in vocab:\n",
        "                matrix[vocab[word]] = vec\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "w2v_weights = load_google_vectors_bin(bin_path, vocab, emb_dim=300)"
      ],
      "metadata": {
        "id": "nZt2seoiKfRy"
      },
      "id": "nZt2seoiKfRy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se construye la matriz **`w2v_weights`** aplicando **`load_google_vectors_bin`** al modelo *Word2Vec GoogleNews*. Este modelo contiene 3 millones de vectores de 300 dimensiones entrenados con el algoritmo skip-gram con negative sampling sobre ≈ 100 mil millones de palabras del corpus Google News. Los autores sustituyeron las redes n-gram tradicionales por dos arquitecturas sencillas (CBOW y Skip-gram) con optimizaciones como negative sampling y sub-sampling de palabras frecuentes. Esto permitió entrenar embeddings de alta calidad en horas, no en días [²].\n",
        "\n",
        "Esto vectores se cracteríza por capturar relaciones aditivas (“king − man + woman ≈ queen”) y mejoraron sustancialmente las tareas de analogías y similitud frente a métodos previos. Es un amplio vocabulario y la granularidad de 300 dimensiones hacen que todavía sea un punto de partida competitivo para tareas de PLN en inglés, sobre todo cuando el corpus propio es pequeño o se busca ahorrar tiempo de entrenamiento."
      ],
      "metadata": {
        "id": "MFBM8AiDXFqo"
      },
      "id": "MFBM8AiDXFqo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nube de palabras"
      ],
      "metadata": {
        "id": "IOYz0UWa9jRl"
      },
      "id": "IOYz0UWa9jRl"
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_wordcloud(X: pd.DataFrame):\n",
        "\n",
        "  # Foo\n",
        "  X_preprocesado = X.text.apply(lambda x: clean_and_tokenize(x))\n",
        "  texto_nube = ' '.join(w for text in X_preprocesado for w in text)\n",
        "\n",
        "  # Generación Nube de Palabras\n",
        "  wordcloud = WordCloud(\n",
        "      width=800,\n",
        "      height=400,\n",
        "      background_color ='white',\n",
        "      min_font_size=10,\n",
        "      max_font_size=110,\n",
        "      max_words=100\n",
        "  ).generate(texto_nube)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "sBwcvN1xtktk"
      },
      "id": "sBwcvN1xtktk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time generar_wordcloud(data)"
      ],
      "metadata": {
        "id": "3Rof6FeuAQNF"
      },
      "id": "3Rof6FeuAQNF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "G6-RrjwuxYEw",
      "metadata": {
        "id": "G6-RrjwuxYEw"
      },
      "source": [
        "***\n",
        "\n",
        "# 4. Desarrollo del modelo RNN...."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 4.1. Hiperparámetros, partición y *DataLoaders*\n",
        "\n",
        "Se crea un **`IMDBDataset`** que toma cada reseña del **`DataFrame`**, la limpia, la tokeniza, la codifica a 200 índices con **`padding`** y entrega la pareja tensor entero + etiqueta flotante , lista para **`nn.BCELoss`**. Con una semilla fija se divide el conjunto en **`80 %–10 %–10 %`** (entrenamiento, validación y prueba) y cada segmento se coloca en un **`DataLoader`** de lote 64, barajado solo en entrenamiento."
      ],
      "metadata": {
        "id": "wqy-xlwJ2zMS"
      },
      "id": "wqy-xlwJ2zMS"
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, df, max_len=200):\n",
        "        self.texts  = df['text'].tolist()\n",
        "        self.labels = df['label'].astype(int).tolist()\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        toks = clean_and_tokenize(self.texts[i])\n",
        "        return torch.tensor(encode(toks,self.max_len)), torch.tensor(self.labels[i],dtype=torch.float)\n",
        "\n",
        "# Split 80/10/10 con semilla fija\n",
        "ds = IMDBDataset(data)\n",
        "n = len(ds)\n",
        "train_ds, val_ds, test_ds = random_split(ds, [int(.8*n),int(.1*n),n-int(.9*n)], generator=torch.Generator().manual_seed(RANDOM_STATE))\n",
        "\n",
        "batch_size = 64 #Batch grande\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "OIoaU3iKH6ey"
      },
      "id": "OIoaU3iKH6ey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`SentimentRNN`**, derivada de **`Module`**, integra cuatro componentes clave en un único flujo [³]. Primero aparece la capa de *embeddings*: si se suministran **`pretrained_weights`**, estos vectores se cargan mediante **`Embedding.from_pretrained`** y pueden mantenerse fijos o descongelarse según convenga; de lo contrario, se crea un **`Embedding`** estándar con el tamaño del vocabulario. A continuación se encuentra la LSTM, configurable en dimensión oculta, número de capas y bidireccionalidad. Cuando se activa el modo bidireccional, el modelo recorre cada secuencia tanto en su orden natural como en sentido inverso, capturando dependencias de corto y largo alcance, y aplica *dropout* interno entre capas para mejorar la generalización.\n",
        "\n",
        "Tras procesar la secuencia, se conserva únicamente el último estado temporal —que ya sintetiza la información más relevante— y se le aplica un **`Dropout`** externo como medida adicional contra el sobre‑ajuste. Finalmente, una capa totalmente conectada reduce la representación a un valor escalar que **`sigmoid`** transforma en la probabilidad de que la reseña sea positiva. El método **`forward`** encadena estos pasos y devuelve el resultado aplanado con **`.squeeze`** para facilitar la comparación con la etiqueta.\n",
        "\n",
        "---\n",
        "\n",
        "La preferencia por LSTM sobre alternativas como GRU responde a tres factores complementarios. Primero, las reseñas de IMDb rondan los 200 *tokens*, por lo que las dependencias de largo plazo resultan importantes y la memoria explícita de la LSTM ofrece una ventaja. Segundo, el tamaño del corpus es lo bastante grande como para entrenar un modelo con más parámetros sin incurrir en sobre‑ajuste excesivo, especialmente con las técnicas de *dropout* ya mencionadas. Tercero y decisivo, un estudio reciente muestra que en este mismo conjunto de datos la LSTM supera consistentemente a la GRU en *accuracy*, corroborando hallazgos anteriores [⁴]. A la luz de esta evidencia y de los requisitos de desempeño del proyecto, la LSTM se presenta como la opción más sólida."
      ],
      "metadata": {
        "id": "vPRMS7KNUVmE"
      },
      "id": "vPRMS7KNUVmE"
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=300, hid_dim=128, n_layers=3, bidir=True, drop=0.6, pretrained_weights=None):\n",
        "        super().__init__()\n",
        "        #Capa de embedding\n",
        "        if pretrained_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.FloatTensor(pretrained_weights),\n",
        "                freeze=True,\n",
        "                padding_idx=0\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        # RNN\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional=bidir, batch_first=True, dropout=drop if n_layers > 1 else 0.0)\n",
        "        # Droput\n",
        "        self.dropout = nn.Dropout(drop)\n",
        "        # Red completamente conectada\n",
        "        self.fc = nn.Linear(hid_dim*(2 if bidir else 1), 1)\n",
        "    def forward(self, x):\n",
        "        emb, _ = self.embedding(x), None\n",
        "        out, _ = self.lstm(emb)\n",
        "        h_last = out[:, -1, :]\n",
        "        return torch.sigmoid(self.fc(self.dropout(h_last))).squeeze()"
      ],
      "metadata": {
        "id": "cipLcGcoKLdm"
      },
      "id": "cipLcGcoKLdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 4.3. Entrenamiento, validación y prueba\n",
        "\n",
        "La función **`train`** recorre las épocas alternando entrenamiento y validación, calcula pérdidas y precisiones, y registra todo en **`history`**. Al inicio se utiliza la técnica de descongelamiento progresivo [⁵] la cual  permite (i) conservar características generales útiles, (ii) adaptar con seguridad las capas superiores y (iii) mejorar la precisión sin sobre-ajustar, sobre todo en conjuntos de datos pequeños o medianos. Esta técnica se aplica a partir de la época 5 donde se llama a **`requires_grad_(True)`** y se crea un nuevo optimizador para incluir los parámetros recién liberados; esto permite fine-tuning de los vectores sin alterar la fase de arranque.\n",
        "\n",
        "Antes de cada actualización se aplica **`gradient clipping`** mediante **`clip_grad_norm_`**, limitando la norma de los gradientes a **`1.0`**; esto previene el problema de **`exploding gradients`**, que puede desestabilizar el aprendizaje al producir actualizaciones desmesuradas en los pesos, sobre todo en redes recurrentes y arquitecturas profundas [⁶]. Tras cada época se guarda el mejor modelo según la pérdida de validación y se activa **`Early Stopping`** si no hay mejora durante el número de épocas definido por **`patience`**; al finalizar, la función devuelve **`history`** para análisis posterior."
      ],
      "metadata": {
        "id": "oz4ZPXG44a8W"
      },
      "id": "oz4ZPXG44a8W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrena una red para clasificación binaria con Early Stopping y gradient clipping.\n",
        "def train( model,train_loader,val_loader,epochs=10,lr=1e-3,device=\"cuda\",clip=1.0,patience=5, save_path=\"best_model.pt\"):\n",
        "\n",
        "    criterion  = nn.BCELoss()\n",
        "    optimizer  = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-2)  # <— usa el lr pasado\n",
        "    best_val   = float(\"inf\")\n",
        "    epochs_no_imp = 0\n",
        "\n",
        "    history = {k: [] for k in (\"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\")}\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Descongelar embeddings en la época 5\n",
        "        if epoch == 5:\n",
        "            model.embedding.weight.requires_grad_(True)\n",
        "\n",
        "            # opción simple: recrea el optimizador para que incluya la capa\n",
        "            optimizer = optim.Adam(\n",
        "                model.parameters(), lr=lr, weight_decay=1e-2\n",
        "            )\n",
        "\n",
        "        # ---------- Fase de entrenamiento ----------\n",
        "        model.train()\n",
        "        running_loss, running_corrects, total_train = 0.0, 0, 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss    = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)  # gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss     += loss.item() * x.size(0)\n",
        "            preds             = (outputs >= 0.5).float()\n",
        "            running_corrects += (preds == y).sum().item()\n",
        "            total_train      += x.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / total_train\n",
        "        epoch_train_acc  = running_corrects / total_train\n",
        "\n",
        "        # ---------- Fase de validación ----------\n",
        "        model.eval()\n",
        "        val_loss, val_corrects, total_val = 0.0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                outputs = model(x)\n",
        "                val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "\n",
        "                preds        = (outputs >= 0.5).float()\n",
        "                val_corrects += (preds == y).sum().item()\n",
        "                total_val    += x.size(0)\n",
        "\n",
        "        epoch_val_loss = val_loss / total_val\n",
        "        epoch_val_acc  = val_corrects / total_val\n",
        "\n",
        "        # ---------- Registro de métricas ----------\n",
        "        history[\"train_loss\"].append(epoch_train_loss)\n",
        "        history[\"val_loss\"].append(epoch_val_loss)\n",
        "        history[\"train_acc\"].append(epoch_train_acc)\n",
        "        history[\"val_acc\"].append(epoch_val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train Loss {epoch_train_loss:.4f}  Acc {epoch_train_acc:.4f} | \"\n",
        "            f\"Val Loss {epoch_val_loss:.4f}  Acc {epoch_val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # ---------- Early Stopping ----------\n",
        "        if epoch_val_loss < best_val:\n",
        "            best_val = epoch_val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            epochs_no_imp = 0\n",
        "        else:\n",
        "            epochs_no_imp += 1\n",
        "            if epochs_no_imp >= patience:\n",
        "                print(\"→ Early stopping\")\n",
        "                break\n",
        "\n",
        "    mins, secs = divmod(time.time() - start_time, 60)\n",
        "    print(f\"\\nTiempo total: {int(mins)} min {int(secs)} seg\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "DVO5O3T1Kh4O"
      },
      "id": "DVO5O3T1Kh4O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se instancia **`model_w2v`** como una versión de **`SentimentRNN`** cuyos vectores de entrada provienen de un embebido **`Word2Vec`** de 300 dimensiones (**`w2v_weights`**) y se mueve al **`device`** seleccionado (GPU o CPU). Con este modelo se llama a **`train`** durante 15 épocas, una tasa de aprendizaje de **`2 × 10⁻⁴`**, *gradient clipping* y *early stopping* habilitados, utilizando los **`DataLoaders`** de entrenamiento y validación. El historial de pérdidas y precisiones se almacena en **`hist_w2v`**, mientras que los mejores pesos (según la pérdida de validación más baja) se guardan en **`best_w2v.pt`** para uso posterior."
      ],
      "metadata": {
        "id": "75G3J6o5ieVc"
      },
      "id": "75G3J6o5ieVc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "model_w2v = SentimentRNN(\n",
        "    vocab_size=len(vocab),\n",
        "    emb_dim=300,\n",
        "    hid_dim=256,\n",
        "    n_layers=3,\n",
        "    drop=0.6,\n",
        "    pretrained_weights=w2v_weights\n",
        ").to(device)\n",
        "\n",
        "hist_w2v = train(\n",
        "    model_w2v,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=15,\n",
        "    lr=2e-4,\n",
        "    device=device,\n",
        "    save_path=\"best_w2v.pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZYBhqK0KKzNt"
      },
      "id": "ZYBhqK0KKzNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7JgtiyyfIyMM",
      "metadata": {
        "id": "7JgtiyyfIyMM"
      },
      "source": [
        "***\n",
        "\n",
        "\n",
        "# 5. Análisis de resultados y conclusiones\n",
        "\n",
        "Se instancia un nuevo **`SentimentRNN`** con los pesos de **`Word2Vec`** (300 dimensiones) y se envía al **`device`** detectado; a continuación se cargan los parámetros previamente entrenados desde **`best_w2v.pt`** mediante **`load_state_dict(torch.load(...))`**, dejando al modelo **`best_w2v`** listo para inferencia o evaluación sin requerir más entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_w2v = SentimentRNN(len(vocab), emb_dim=300, pretrained_weights=w2v_weights).to(device)\n",
        "best_w2v.load_state_dict(torch.load(\"best_w2v.pt\"))"
      ],
      "metadata": {
        "id": "MwXrB3ThRHcI"
      },
      "id": "MwXrB3ThRHcI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la función **`plot_history`** que recibe el diccionario **`history`** y crea dos gráficos de línea con **`matplotlib`**: el primero compara **`train_loss`** y **`val_loss`** por época, y el segundo hace lo mismo con **`train_acc`** y **`val_acc`**."
      ],
      "metadata": {
        "id": "Ty3gzWEui8xK"
      },
      "id": "Ty3gzWEui8xK"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Pérdidas\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs, history['val_loss'],   label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Exactitudes\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(epochs, history['train_acc'], label='Train Acc')\n",
        "    plt.plot(epochs, history['val_acc'],   label='Val Acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d4PT-Gz1Y9Mh"
      },
      "id": "d4PT-Gz1Y9Mh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(hist_w2v)"
      ],
      "metadata": {
        "id": "hkHiJTa-ZAFC"
      },
      "id": "hkHiJTa-ZAFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El gráfico muestra cómo la pérdida de entrenamiento (**`Train Loss`**) desciende de manera casi monótona, mientras que la pérdida de validación (**`Val Loss`**) baja lentamente hasta la época 5, oscila con un pico pronunciado en la época 9 y alcanza su mínimo en la 10, punto en el que se reduce la brecha entre ambas curvas. A partir de ahí la **`Train Loss`** sigue disminuyendo, pero la **`Val Loss`** se estabiliza y vuelve a subir levemente, señal de sobreajuste. El uso de *early stopping* detuvo el entrenamiento cerca del óptimo de validación, evitando que el modelo se desvíe; sin embargo, la variabilidad previa sugiere que podrían mejorarse la regularización (más **`dropout`** o **`weight decay`**) o la diversidad de datos para lograr curvas de validación más suaves y una convergencia más estable."
      ],
      "metadata": {
        "id": "qJZ_NDzdjQ7G"
      },
      "id": "qJZ_NDzdjQ7G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se cargan los pesos óptimos en **`model_w2v`** con **`load_state_dict`** y se cambia a modo evaluación mediante **`eval()`** para desactivar **`dropout`** y el cálculo de gradientes. A continuación se recorre el **`test_loader`** sin seguimiento de gradiente (**`torch.no_grad()`**); cada lote se envía al **`device`**, se obtiene la probabilidad de clase positiva, se aplica un umbral de **`0.5`** para convertirla en etiqueta binaria y se acumulan predicciones (**`y_pred`**) y verdaderos (**`y_true`**). Finalmente se calculan cuatro métricas de rendimiento: **`accuracy_score`** (proporción de aciertos globales), **`recall_score`** (sensibilidad hacia la clase positiva), **`f1_score`** (promedio armónico entre precisión y _recall_) y **`confusion_matrix`** (tabla de errores y aciertos), proporcionando una visión cuantitativa de la capacidad generalizadora del modelo entrenado."
      ],
      "metadata": {
        "id": "We4fOjKpj4_8"
      },
      "id": "We4fOjKpj4_8"
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.load_state_dict(torch.load('best_w2v.pt'))\n",
        "model_w2v.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        out = (model_w2v(x.to(device)).cpu().numpy() >= 0.5).astype(int)\n",
        "        y_pred.extend(out.tolist())\n",
        "        y_true.extend(y.numpy().astype(int).tolist())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true,y_pred))\n",
        "print(\"Recall:\", recall_score(y_true,y_pred))\n",
        "print(\"F1‑score:\", f1_score(y_true,y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true,y_pred))"
      ],
      "metadata": {
        "id": "QI42wZnGNDhC"
      },
      "id": "QI42wZnGNDhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm      = confusion_matrix(y_true, y_pred)\n",
        "class_names = ['Negative', 'Positive']\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel(\"Predicción\")\n",
        "plt.ylabel(\"Etiqueta real\")\n",
        "plt.title(\"Matriz de confusión — Conjunto de prueba\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VY-_TQIJkJbw"
      },
      "id": "VY-_TQIJkJbw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo alcanza una **`accuracy`** del **`0.878`**, lo que significa que casi 88 % de las reseñas se clasifican correctamente. Con un **`recall`** de **`0.850`**, identifica alrededor del 85 % de las reseñas realmente positivas; unido a un **`F1-score`** de **`0.875`**, esto indica un buen equilibrio entre precisión y recuperación de la clase positiva.\n",
        "\n",
        "La matriz de confusión muestra que el modelo confunde más frecuentemente positivas como negativas (301 FN) que al revés (184 FP). Aun así, la proporción de verdaderos positivos (1700) y verdaderos negativos (1788) domina el total, evidenciando un desempeño sólido pero con margen para reducir los **`false negatives`**—p. ej., ajustando el umbral de decisión o añadiendo regularización para mejorar la cobertura de la clase positiva."
      ],
      "metadata": {
        "id": "ZCYFkXAEkRND"
      },
      "id": "ZCYFkXAEkRND"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función **`predict_sentiment`** recibe un texto libre y, sin necesidad de gradientes, lo procesa en cinco pasos: primero lo limpia y tokeniza con **`clean_and_tokenize`**; luego transforma cada token en su índice mediante el **`vocab`**, rellenando con **`<PAD>`** o truncando hasta **`sequence_length` = 200**. La secuencia resultante se convierte en un tensor de lote 1, se envía al **`device`** (CPU/GPU) y se pasa por el modelo **`SentimentRNN`** en modo evaluación, obteniendo una probabilidad entre 0 y 1. Finalmente imprime el **`Score de positividad`** y muestra un mensaje 🟢 o 🔴 según si la probabilidad supera el umbral de 0.5, permitiendo clasificar rápidamente cualquier reseña como positiva o negativa."
      ],
      "metadata": {
        "id": "i7hL2aoJkok-"
      },
      "id": "i7hL2aoJkok-"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(review_text: str,\n",
        "                      model: nn.Module,\n",
        "                      vocab: dict,\n",
        "                      sequence_length: int = 200,\n",
        "                      device: torch.device = device) -> None:\n",
        "    \"\"\"\n",
        "    Usa el modelo entrenado para predecir si `review_text` es positiva o negativa.\n",
        "    - review_text: cadena con la reseña a clasificar.\n",
        "    - model: instancia de SentimentRNN ya entrenada.\n",
        "    - vocab: diccionario palabra->índice.\n",
        "    - sequence_length: longitud fija del input (padding/truncado).\n",
        "    - device: 'cuda' o 'cpu'.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1) Tokenizar y limpiar\n",
        "    tokens = clean_and_tokenize(review_text)\n",
        "\n",
        "    # 2) Codificar + pad/truncado\n",
        "    idxs = [vocab.get(t, vocab['<UNK>']) for t in tokens]\n",
        "    if len(idxs) < sequence_length:\n",
        "        idxs += [vocab['<PAD>']] * (sequence_length - len(idxs))\n",
        "    else:\n",
        "        idxs = idxs[:sequence_length]\n",
        "\n",
        "    # 3) Tensor batch_size=1 y mover a device\n",
        "    x = torch.tensor([idxs], dtype=torch.long).to(device)\n",
        "\n",
        "    # 4) Forward sin gradientes\n",
        "    with torch.no_grad():\n",
        "        prob = model(x).item()  # valor [0,1]\n",
        "\n",
        "    # 5) Mostrar resultados\n",
        "    print(f\"➡️  Score de positividad: {prob:.4f}\")\n",
        "    if prob >= 0.5:\n",
        "        print(\"🟢  Positive review detected!\")\n",
        "    else:\n",
        "        print(\"🔴  Negative review detected!\")\n"
      ],
      "metadata": {
        "id": "tbtQicsqWwTa"
      },
      "id": "tbtQicsqWwTa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"Thanks to this movie we are all dumber than before.\"\n",
        "predict_sentiment(review, model_w2v, vocab)"
      ],
      "metadata": {
        "id": "hs_gr1QGW2D5"
      },
      "id": "hs_gr1QGW2D5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La reseña  recibió un **`score de positividad`** de **`0.215`**, muy por debajo del umbral **`0.5`**, por lo que el modelo la clasifica como **`Negative review`** con alta confianza. La probabilidad refleja que los términos «dumber» y la construcción irónica «Thanks to this movie…» transmiten un juicio claramente desfavorable, pero que un modelo básico podría confundir al valorar positivamente la palabra *thanks*. El LSTM ha aprendido a asociar esta combinación de palabras con la clase negativa. Si se quisiera afinar la detección de sarcasmo o matices sutiles, podría bajarse el umbral o incorporar ejemplos etiquetados de lenguaje sarcástico para mejorar la sensibilidad en este tipo de frases."
      ],
      "metadata": {
        "id": "ONaHERJlkywv"
      },
      "id": "ONaHERJlkywv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "\n",
        "# 6. Conclusión\n",
        "\n",
        "El clasificador basado en **`SentimentRNN + Word2Vec`** alcanza un rendimiento sólido (≈ 88 % de *accuracy* y F1 ≈ 0.88), aunque aún comete más falsos negativos que falsos positivos. La prueba con la reseña “Thanks to this movie we are all dumber than before.” confirma que el modelo detecta críticas evidentes. Para mejorar, convendría (i) ajustar el umbral o ponderar la clase positiva para reducir falsos negativos, (ii) reforzar la regularización (*dropout*, *weight decay*) y explorar búsqueda de hiperparámetros, y (iii) ampliar el repertorio de ejemplos —incluido sarcasmo— o probar representaciones más ricas (p. ej., embeddings contextualizados o atención) que capten mejor los matices lingüísticos."
      ],
      "metadata": {
        "id": "uDjo26cIk10k"
      },
      "id": "uDjo26cIk10k"
    },
    {
      "cell_type": "markdown",
      "id": "DwUWQIAE3O0o",
      "metadata": {
        "id": "DwUWQIAE3O0o"
      },
      "source": [
        "***\n",
        "\n",
        "\n",
        "# 7. Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BoLDjBS03xiQ",
      "metadata": {
        "id": "BoLDjBS03xiQ"
      },
      "source": [
        "\n",
        "[¹] **Pytorch, Sentiment Analysis**  \n",
        "Disponible en: [github.com](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/2%20-%20Recurrent%20Neural%20Networks.ipynb)\n",
        "\n",
        "[¹] **GoogleNews-vectors-negative300**  \n",
        "Disponible en: [kaggle.com](https://www.kaggle.com/datasets/adarshsng/googlenewsvectors)\n",
        "\n",
        "\n",
        "[³] **Sentiment Analysis with an RNN**  \n",
        "Disponible en: [github.com](https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=TJHNs4FZpmwj)\n",
        "\n",
        "\n",
        "\n",
        "[⁴] **LSTM and GRU Neural Network Performance Comparison Study: Taking Yelp Review Dataset as an Example**  \n",
        "Disponible en: [researchgate.net/](https://www.researchgate.net/publication/347267378_LSTM_and_GRU_Neural_Network_Performance_Comparison_Study_Taking_Yelp_Review_Dataset_as_an_Example)\n",
        "\n",
        "\n",
        "[⁵] **Transfer Learning: Why We Freeze and Unfreeze Model Layers**  \n",
        "Disponible en: [medium.com](https://medium.com/data-science-collective/transfer-learning-why-we-freeze-and-unfreeze-model-layers-0e0b8f9837ec)\n",
        "\n",
        "[⁶] **Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)**  \n",
        "Disponible en: [neptune.ai](https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem?utm_source=chatgpt.com)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}